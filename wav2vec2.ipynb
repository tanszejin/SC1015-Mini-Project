{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Wav2Vec2 Transformer\n",
    "\n",
    "Wav2Vec2 is a speech model that accepts a float array corresponding to the raw waveform of the speech signal.\n",
    "\n",
    "In this notebook, we will train the Wav2Vec2 base model, built on the Hugging Face Transformers library, to determine the instrument present in an audio segment."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import Wav2Vec2ForSequenceClassification, Wav2Vec2Processor, AdamW\n",
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader, ConcatDataset\n",
    "import torchaudio\n",
    "from torchaudio.transforms import Resample\n",
    "from pathlib import Path\n",
    "\n",
    "from datetime import datetime\n",
    "from IPython.display import clear_output"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Set Parameters for Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "target_duration = 1 # 1 second"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define the Dataset\n",
    "\n",
    "The `AudioDataset` class is designed to handle the loading and preprocessing of audio files for the model.\n",
    "\n",
    "The only preprocessing done is the resampling of the audio file if its sampling rate is not already 16000Hz."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class AudioDataset(Dataset):\n",
    "    total_files_loaded = 0  # Class attribute to count files loaded across all instances\n",
    "\n",
    "    def __init__(self, base_path, class_name, processor, class_names, target_duration=target_duration): # 1 second\n",
    "        self.base_path = Path(base_path)\n",
    "        self.class_name = class_name\n",
    "        self.processor = processor\n",
    "        self.class_names = class_names\n",
    "        self.target_duration = target_duration\n",
    "        self.label_id = self.class_names.index(class_name)  # Get numerical ID\n",
    "        self.audios = list((self.base_path / self.class_name).glob('*.wav'))\n",
    "    \n",
    "    def __len__(self):\n",
    "        length = len(self.audios)\n",
    "        print(f\"Reported dataset length: {length}\")\n",
    "        return length\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        if idx >= len(self.audios):\n",
    "            raise IndexError(f\"Index {idx} out of range for dataset with length {len(self.audios)}.\")\n",
    "        audio_path = self.audios[idx]\n",
    "\n",
    "        AudioDataset.total_files_loaded += 1\n",
    "        print(f\"Status: {AudioDataset.total_files_loaded}/{len(self.audios)*3}\\t{self.class_name},\\t Accessing index: {idx},\\t Audio path: {audio_path}\")\n",
    "        \n",
    "        waveform, sample_rate = torchaudio.load(audio_path)\n",
    "\n",
    "        # Preprocessing\n",
    "        if sample_rate != 16000:\n",
    "            resampler = Resample(orig_freq=sample_rate, new_freq=16000)\n",
    "            waveform = resampler(waveform)\n",
    "\n",
    "        # input feature\n",
    "        input_values = self.processor(waveform, sampling_rate=16000, return_tensors=\"pt\").input_values\n",
    "        \n",
    "        return {\"input_values\": input_values.squeeze(), \"labels\": torch.tensor(self.label_id)}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Initialize Data Loaders\n",
    "\n",
    "Set up the data loaders required for training and validation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\heidi\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\transformers\\configuration_utils.py:363: UserWarning: Passing `gradient_checkpointing` to a config initialization is deprecated and will be removed in v5 Transformers. Using `model.gradient_checkpointing_enable()` instead, or if you are using the `Trainer` API, pass `gradient_checkpointing=True` in your `TrainingArguments`.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reported dataset length: 1597\n",
      "Reported dataset length: 1597\n",
      "Reported dataset length: 1597\n",
      "Reported dataset length: 143\n",
      "Reported dataset length: 407\n",
      "Reported dataset length: 92\n"
     ]
    }
   ],
   "source": [
    "# Configuration and setup\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "processor = Wav2Vec2Processor.from_pretrained(\"facebook/wav2vec2-base\")\n",
    "\n",
    "# Paths for datasets\n",
    "base_train_path = 'solo_train_data/split'\n",
    "base_val_path = 'solo_test_data/split'\n",
    "\n",
    "# Classes\n",
    "class_names = [\"Cello\", \"Piano\", \"Violin\"]\n",
    "\n",
    "# Instantiate datasets for each class\n",
    "train_datasets = {}\n",
    "val_datasets = {}\n",
    "\n",
    "# Create datasets\n",
    "for class_name in class_names:\n",
    "  train_datasets[class_name] = AudioDataset(base_train_path, class_name, processor, class_names)\n",
    "  val_datasets[class_name] = AudioDataset(base_val_path, class_name, processor, class_names)\n",
    "\n",
    "# Concatenate datasets into a single dataset for training and validation\n",
    "train_dataset = ConcatDataset(list(train_datasets.values()))\n",
    "val_dataset = ConcatDataset(list(val_datasets.values()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Set Parameters for Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training Parameters\n",
    "num_epochs = 200\n",
    "batch_size = 64\n",
    "num_labels = 3  # Cello, Piano, Violin"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load the Base Model, Create DataLoader, and Set the Model Optimiser\n",
    "\n",
    "AdamW is a variant of the Adam optimizer with modifications for better handling of weight decay. <br>\n",
    "It is necessary in this project because it decouples weight decay from the gradient updates, providing better regularization and often leading to improved training convergence, especially beneficial when fine-tuning pre-trained deep learning models. This optimizer is particularly effective in preventing overfitting and ensuring more stable updates."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of Wav2Vec2ForSequenceClassification were not initialized from the model checkpoint at facebook/wav2vec2-base and are newly initialized: ['classifier.bias', 'classifier.weight', 'projector.bias', 'projector.weight', 'wav2vec2.encoder.pos_conv_embed.conv.parametrizations.weight.original0', 'wav2vec2.encoder.pos_conv_embed.conv.parametrizations.weight.original1']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "c:\\Users\\heidi\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\transformers\\optimization.py:521: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "# Load the processor and the base model\n",
    "model = Wav2Vec2ForSequenceClassification.from_pretrained(\"facebook/wav2vec2-base\", num_labels=num_labels)\n",
    "model.to(device)\n",
    "\n",
    "# Create DataLoaders\n",
    "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True, num_workers=6, pin_memory=True, persistent_workers=True)\n",
    "val_loader = DataLoader(val_dataset, batch_size=batch_size, num_workers=6, pin_memory=True, persistent_workers=True)\n",
    "\n",
    "# Optimizer\n",
    "optimiser = AdamW(model.parameters(), lr=5e-5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training the Model\n",
    "\n",
    "Training of 1 epoch takes about 3 hours on a Ryzen 3600 CPU.\n",
    "\n",
    "If the code for training does not work in the notebook, please use the Python file `model_training`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "best_val_accuracy = 0.0\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "  # Training phase\n",
    "  model.train()\n",
    "  for batch in train_loader:\n",
    "    optimiser.zero_grad()\n",
    "    input_values = batch[\"input_values\"].to(device)\n",
    "    labels = batch[\"labels\"].to(device)\n",
    "    outputs = model(input_values=input_values, labels=labels)\n",
    "    loss = outputs.loss\n",
    "    loss.backward()\n",
    "    optimiser.step()\n",
    "  \n",
    "  # Evaluation phase\n",
    "  model.eval()\n",
    "  correct_predictions = 0\n",
    "  total_predictions = 0\n",
    "  total_loss = 0\n",
    "  with torch.no_grad():\n",
    "    for batch in val_loader:\n",
    "      input_values = batch[\"input_values\"].to(device)\n",
    "      labels = batch[\"labels\"].to(device)\n",
    "      outputs = model(input_values=input_values, labels=labels)\n",
    "      loss = outputs.loss\n",
    "      total_loss += loss.item()\n",
    "      \n",
    "      # Calculate accuracy\n",
    "      predictions = torch.argmax(outputs.logits, dim=-1)\n",
    "      correct_predictions += (predictions == labels).sum().item()\n",
    "      total_predictions += labels.size(0)\n",
    "\n",
    "  val_accuracy = correct_predictions / total_predictions\n",
    "  val_loss = total_loss / len(val_loader)\n",
    "\n",
    "  print(f\"Epoch {epoch+1}, Loss: {val_loss:.4f}, Accuracy: {val_accuracy:.4f}\")\n",
    "  \n",
    "  # Save the model if it has the best accuracy so far\n",
    "  if val_accuracy > best_val_accuracy:\n",
    "    best_val_accuracy = val_accuracy\n",
    "    # Create a directory to save the model, using the current date and time for uniqueness\n",
    "    current_datetime = datetime.now().strftime('%Y-%m-%d_%H-%M-%S')\n",
    "    model_save_path = f'model/{current_datetime}_{val_accuracy:.4f}.bin'\n",
    "    torch.save(model.state_dict(), model_save_path)\n",
    "    print(f\"New best model saved with accuracy: {val_accuracy:.4f} at '{model_save_path}'\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
