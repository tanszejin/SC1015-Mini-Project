{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Testing the Trained Wav2Vec2 Model\n",
    "\n",
    "We will be testing the model using 2 methods: <br>\n",
    "- live audio from your microphone\n",
    "- upload a .wav file"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import Libaries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loading the model\n",
    "from transformers import Wav2Vec2Processor, Wav2Vec2ForSequenceClassification, Wav2Vec2Config\n",
    "import torch\n",
    "\n",
    "# For recording using your microphone\n",
    "import sounddevice as sd\n",
    "\n",
    "# For managing uploaded audio\n",
    "import torchaudio\n",
    "\n",
    "# Display classification output\n",
    "from tabulate import tabulate\n",
    "import time\n",
    "from IPython.display import clear_output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup Model and Processor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\heidi\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\transformers\\configuration_utils.py:363: UserWarning: Passing `gradient_checkpointing` to a config initialization is deprecated and will be removed in v5 Transformers. Using `model.gradient_checkpointing_enable()` instead, or if you are using the `Trainer` API, pass `gradient_checkpointing=True` in your `TrainingArguments`.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "# Class names\n",
    "class_names = [\"Cello\", \"Piano\", \"Violin\"]\n",
    "\n",
    "# Path to your model\n",
    "model_path = \"model/2024-03-31_23-47-51_0.9984.bin\"\n",
    "\n",
    "# Load model and processor\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "config = Wav2Vec2Config.from_pretrained(\"facebook/wav2vec2-base\", num_labels=len(class_names))\n",
    "model = Wav2Vec2ForSequenceClassification(config)\n",
    "model.load_state_dict(torch.load(model_path, map_location=device))\n",
    "model.to(device)\n",
    "model.eval()\n",
    "\n",
    "processor = Wav2Vec2Processor.from_pretrained(\"facebook/wav2vec2-base\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preprocess input audio\n",
    "\n",
    "The function below preprocess the audio before it is input into the model.\n",
    "\n",
    "The function ensure that the audio is mono channel (1 channel) and it is sampled in 16000Hz."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_audio(recording, processor, sampling_rate=16000):\n",
    "  # Ensure recording is 1D for single channel\n",
    "  if recording.ndim > 1:\n",
    "      recording = recording.squeeze()\n",
    "  \n",
    "  # Check length of recording, pad if necessary\n",
    "  min_length = 16000  # Minimum length required by the model\n",
    "  if len(recording) < min_length:\n",
    "    # Pad the recording if it's too short\n",
    "    pad_amount = min_length - len(recording)\n",
    "    recording = torch.nn.functional.pad(recording, (0, pad_amount), \"constant\", 0)\n",
    "  \n",
    "  input_values = processor(recording, sampling_rate=sampling_rate, return_tensors=\"pt\", padding=True).input_values\n",
    "  return input_values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Classify/Predict the Audio Input\n",
    "\n",
    "This function uses the model to predict the instrument present in the input audio.\n",
    "\n",
    "It returns the inference time along with the classification probabilities of the input."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict(model, processor, input_values, device):\n",
    "    input_values = input_values.to(device)\n",
    "\n",
    "    start_time = time.time()  # Step 2: Record the start time\n",
    "    with torch.no_grad():\n",
    "        logits = model(input_values).logits\n",
    "    end_time = time.time()  # Step 3: Record the end time\n",
    "    inference_time = end_time - start_time  # Step 4: Calculate the inference time\n",
    "\n",
    "    # Compute softmax probabilities\n",
    "    probabilities = torch.nn.functional.softmax(logits, dim=-1)\n",
    "    return probabilities, inference_time  # Step 5: Return the inference time along with the probabilities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Predicting Live Audio\n",
    "\n",
    "Use this model to classify the instrument used to produce the audio.\n",
    "\n",
    "The code below records a 1-second audio from your microphone and produces a prediction after every recording."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### `record_audio` function\n",
    "\n",
    "Helper function that records an audio based on the input duration and sampling rate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def record_audio(duration, sampling_rate=16000):\n",
    "  print(\"Recording...\")\n",
    "  recording = sd.rec(int(duration * sampling_rate), samplerate=sampling_rate, channels=1, dtype='float32')\n",
    "  sd.wait()  # Wait until recording is finished\n",
    "  return recording"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### `Live Prediction` function\n",
    "\n",
    "Helper function that classifies the recorded audio into one of the 3 instruments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def live_prediction():\n",
    "    duration=1\n",
    "    sampling_rate=16000\n",
    "\n",
    "    try:\n",
    "        while True:\n",
    "            # Record and preprocess audio\n",
    "            start_time = time.time()\n",
    "            recording = record_audio(duration, sampling_rate)\n",
    "            end_time = time.time()\n",
    "            record_time = end_time - start_time\n",
    "\n",
    "            start_time = time.time()\n",
    "            input_values = preprocess_audio(recording, processor, sampling_rate)\n",
    "            end_time = time.time()\n",
    "            preprocess_time = end_time - start_time\n",
    "            \n",
    "            # Predict and get probabilities\n",
    "            probabilities, inference_time = predict(model, processor, input_values, device)\n",
    "            predicted_prob, predicted_index = torch.max(probabilities, dim=1)\n",
    "            predicted_class = class_names[predicted_index.item()]\n",
    "\n",
    "            # Convert probabilities to percentages and prepare table data\n",
    "            percentages = [prob.item() * 100 for prob in probabilities[0]]\n",
    "            table_data = [[class_name, f\"{percentage:.2f}%\"] for class_name, percentage in zip(class_names, percentages)]\n",
    "\n",
    "            # Print the table using tabulate\n",
    "            clear_output(wait=True)\n",
    "            print(tabulate(table_data, headers=['Class', 'Probability'], tablefmt='grid'))\n",
    "            print(f\"Predicted class: {predicted_class}\\n\")\n",
    "            print(f\"Record Time: {record_time:.4f} seconds\")  # Display the inference time\n",
    "            print(f\"Preprocess Time: {preprocess_time:.4f} seconds\")\n",
    "            print(f\"Inference Time: {inference_time:.4f} seconds\")\n",
    "            print(\"\\nPress the Interrupt button to stop\")\n",
    "    except KeyboardInterrupt:\n",
    "        print(\"Live prediction session ended.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Implement the Live Prediction!\n",
    "\n",
    "Press the interrupt button to stop predicting."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+---------------+\n",
      "| Class   | Probability   |\n",
      "+=========+===============+\n",
      "| Cello   | 18.32%        |\n",
      "+---------+---------------+\n",
      "| Piano   | 69.84%        |\n",
      "+---------+---------------+\n",
      "| Violin  | 11.84%        |\n",
      "+---------+---------------+\n",
      "Predicted class: Piano\n",
      "\n",
      "Record Time: 1.1055 seconds\n",
      "Preprocess Time: 0.0000 seconds\n",
      "Inference Time: 0.1465 seconds\n",
      "\n",
      "Press the Interrupt button to stop\n",
      "Recording...\n",
      "Live prediction session ended.\n"
     ]
    }
   ],
   "source": [
    "live_prediction()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Predicting a .wav Audio File\n",
    "\n",
    "Predict the instrument used in an audio file uploaded by you.\n",
    "\n",
    "The code below will produce a prediction after analysing the whole audio file you uploaded."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### `format_time` function\n",
    "\n",
    "A helper function that converts a time in seconds to minutes and seconds format and rounds seconds to the nearest integer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def format_time(seconds):\n",
    "  minutes = int(seconds // 60)  # Get the full minutes\n",
    "  remaining_seconds = round(seconds % 60)  # Get the remainder seconds, rounded to the nearest integer\n",
    "  return f\"{minutes} minutes and {remaining_seconds} seconds\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### `analyse_wav_file` function\n",
    "\n",
    "A helper function that processes the uploaded audio and produces a culmulative prediction.\n",
    "\n",
    "The first and last 5% of the audio is not analysed as they are usually no instrument playing during these periods."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def analyse_wav_file(wav_path, model, processor, device, segment_duration=1, sampling_rate=16000):\n",
    "  waveform, sr = torchaudio.load(wav_path)\n",
    "  if sr != sampling_rate:\n",
    "    resampler = torchaudio.transforms.Resample(orig_freq=sr, new_freq=sampling_rate)\n",
    "    waveform = resampler(waveform)\n",
    "\n",
    "  if waveform.size(0) > 1:\n",
    "    waveform = torch.mean(waveform, dim=0, keepdim=True)\n",
    "\n",
    "  # Determine the start and end indices to exclude the first and last 5%\n",
    "  total_samples = waveform.size(1)\n",
    "  start_index = int(total_samples * 0.05)  # Start at 5% of the total samples\n",
    "  end_index = int(total_samples * 0.95)    # End at 95% of the total samples\n",
    "\n",
    "  # Adjusted number of samples to process\n",
    "  adjusted_total_samples = end_index - start_index\n",
    "  num_samples = int(sampling_rate * segment_duration)\n",
    "  num_segments = adjusted_total_samples // num_samples\n",
    "\n",
    "  cumulative_probabilities = None\n",
    "\n",
    "  print(\"Processing and predicting segments...\")\n",
    "  for i in range(num_segments):\n",
    "    start_sample = start_index + i * num_samples\n",
    "    end_sample = start_sample + num_samples\n",
    "    segment = waveform[:, start_sample:end_sample]\n",
    "\n",
    "    input_values = preprocess_audio(segment, processor, sampling_rate)\n",
    "    probabilities, _ = predict(model, processor, input_values, device)\n",
    "\n",
    "    if cumulative_probabilities is None:\n",
    "      cumulative_probabilities = probabilities\n",
    "    else:\n",
    "      cumulative_probabilities += probabilities\n",
    "\n",
    "  average_probabilities = cumulative_probabilities / num_segments\n",
    "  average_probabilities = average_probabilities.squeeze()  # Remove unnecessary dimensions\n",
    "\n",
    "  # Identify the final predicted class\n",
    "  max_prob, max_index = torch.max(average_probabilities, dim=0)\n",
    "  final_predicted_class = class_names[max_index.item()]\n",
    "  \n",
    "  # Time information\n",
    "  start_time_seconds = start_index / sampling_rate\n",
    "  end_time_seconds = end_index / sampling_rate\n",
    "  start_time_formatted = format_time(start_time_seconds)\n",
    "  end_time_formatted = format_time(end_time_seconds)\n",
    "\n",
    "  # Prepare data for tabulation\n",
    "  data = [(class_name, f\"{prob * 100:.2f}%\") for class_name, prob in zip(class_names, average_probabilities)]\n",
    "  print(tabulate(data, headers=[\"Class\", \"Probability\"], tablefmt=\"grid\"))\n",
    "  print(f\"Final predicted class: {final_predicted_class} ({max_prob.item() * 100:.2f}%)\")\n",
    "  print(f\"Analysis from {start_time_formatted} to {end_time_formatted}.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Downloading an Audio File\n",
    "\n",
    "Since we do not currently have any audio files, lets download some from YouTube."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Import Libaries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pytube import YouTube\n",
    "from moviepy.editor import AudioFileClip\n",
    "import os"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Helper Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Downloads a YouTube video and returns the download path.\n",
    "def download_video(youtube_url, save_path):\n",
    "  try:\n",
    "    yt = YouTube(youtube_url)\n",
    "    video_stream = yt.streams.filter(only_audio=True).first()\n",
    "    downloaded_file = video_stream.download(output_path=save_path)\n",
    "    return downloaded_file\n",
    "  except Exception as e:\n",
    "    print(f\"Error downloading video: {e}\")\n",
    "    return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Converts the downloaded video to WAV format.\n",
    "def convert_to_wav(video_path, output_path):\n",
    "  try:\n",
    "    video_clip = AudioFileClip(video_path)\n",
    "    video_clip.write_audiofile(output_path, codec='pcm_s16le')\n",
    "    print(f\"Conversion successful. File saved to: {output_path}\")\n",
    "  finally:\n",
    "    video_clip.close()  # Ensure resources are cleaned up"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Deletes the video file specified by video_path.\n",
    "def delete_video_file(video_path):\n",
    "  try:\n",
    "    os.remove(video_path)\n",
    "    print(f\"Deleted video file: {video_path}\")\n",
    "  except Exception as e:\n",
    "    print(f\"Error deleting video file: {e}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Download the Audio of YouTube Videos\n",
    "3 videos are pre-selected to demonstrate the results of the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MoviePy - Writing audio in d:\\_NTU\\Y2S2\\SC1015\\Project\\audio\\Down by the Salley Gardens - Irish Cello.wav\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                      \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MoviePy - Done.\n",
      "Conversion successful. File saved to: d:\\_NTU\\Y2S2\\SC1015\\Project\\audio\\Down by the Salley Gardens - Irish Cello.wav\n",
      "Deleted video file: d:\\_NTU\\Y2S2\\SC1015\\Project\\audio\\Down by the Salley Gardens - Irish Cello.mp4\n",
      "MoviePy - Writing audio in d:\\_NTU\\Y2S2\\SC1015\\Project\\audio\\David Lanz performs Cristoforis Dream live solo piano concert at Piano Haven.wav\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                      \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MoviePy - Done.\n",
      "Conversion successful. File saved to: d:\\_NTU\\Y2S2\\SC1015\\Project\\audio\\David Lanz performs Cristoforis Dream live solo piano concert at Piano Haven.wav\n",
      "Deleted video file: d:\\_NTU\\Y2S2\\SC1015\\Project\\audio\\David Lanz performs Cristoforis Dream live solo piano concert at Piano Haven.mp4\n",
      "MoviePy - Writing audio in d:\\_NTU\\Y2S2\\SC1015\\Project\\audio\\Homelanders Theme I can do anything finale violin performance.wav\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                      "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MoviePy - Done.\n",
      "Conversion successful. File saved to: d:\\_NTU\\Y2S2\\SC1015\\Project\\audio\\Homelanders Theme I can do anything finale violin performance.wav\n",
      "Deleted video file: d:\\_NTU\\Y2S2\\SC1015\\Project\\audio\\Homelanders Theme I can do anything finale violin performance.mp4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r"
     ]
    }
   ],
   "source": [
    "output_folder = \"audio\"\n",
    "if not os.path.exists(output_folder):\n",
    "  os.makedirs(output_folder)\n",
    "\n",
    "youtube_urls = [\n",
    "  \"https://www.youtube.com/watch?v=hykHZOXV-S0\",  #cello\n",
    "  \"https://www.youtube.com/watch?v=q7mgrcULIbs\",  #piano\n",
    "  \"https://www.youtube.com/watch?v=xZZFU0KVpKE\",  #violin\n",
    "]\n",
    "\n",
    "for url in youtube_urls:\n",
    "  video_path = download_video(url,output_folder)\n",
    "  if video_path:\n",
    "    output_path = os.path.join(output_folder, video_path.replace('.mp4', '.wav'))  # Change extension to .wav\n",
    "    convert_to_wav(video_path, output_path)\n",
    "    delete_video_file(video_path)  # Delete the original video file after conversion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Implement the Audio File Prediction!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Cello"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing and predicting segments...\n",
      "+---------+---------------+\n",
      "| Class   | Probability   |\n",
      "+=========+===============+\n",
      "| Cello   | 53.25%        |\n",
      "+---------+---------------+\n",
      "| Piano   | 31.26%        |\n",
      "+---------+---------------+\n",
      "| Violin  | 15.49%        |\n",
      "+---------+---------------+\n",
      "Final predicted class: Cello (53.25%)\n",
      "Analysis from 0 minutes and 11 seconds to 3 minutes and 33 seconds.\n"
     ]
    }
   ],
   "source": [
    "# Select the audio file\n",
    "wav_path = 'audio/Down by the Salley Gardens - Irish Cello.wav'\n",
    "\n",
    "# Analyse the audio file\n",
    "analyse_wav_file(wav_path, model, processor, device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Piano"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing and predicting segments...\n",
      "+---------+---------------+\n",
      "| Class   | Probability   |\n",
      "+=========+===============+\n",
      "| Cello   | 11.85%        |\n",
      "+---------+---------------+\n",
      "| Piano   | 78.62%        |\n",
      "+---------+---------------+\n",
      "| Violin  | 9.53%         |\n",
      "+---------+---------------+\n",
      "Final predicted class: Piano (78.62%)\n",
      "Analysis from 0 minutes and 19 seconds to 6 minutes and 10 seconds.\n"
     ]
    }
   ],
   "source": [
    "# Select the audio file\n",
    "wav_path = 'audio/David Lanz performs Cristoforis Dream live solo piano concert at Piano Haven.wav'\n",
    "\n",
    "# Analyse the audio file\n",
    "analyse_wav_file(wav_path, model, processor, device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Violin"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing and predicting segments...\n",
      "+---------+---------------+\n",
      "| Class   | Probability   |\n",
      "+=========+===============+\n",
      "| Cello   | 9.83%         |\n",
      "+---------+---------------+\n",
      "| Piano   | 9.06%         |\n",
      "+---------+---------------+\n",
      "| Violin  | 81.11%        |\n",
      "+---------+---------------+\n",
      "Final predicted class: Violin (81.11%)\n",
      "Analysis from 0 minutes and 3 seconds to 1 minutes and 1 seconds.\n"
     ]
    }
   ],
   "source": [
    "# Select the audio file\n",
    "wav_path = 'audio/Homelanders Theme I can do anything finale violin performance.wav'\n",
    "\n",
    "# Analyse the audio file\n",
    "analyse_wav_file(wav_path, model, processor, device)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
